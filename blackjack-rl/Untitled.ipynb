{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a8d920-ba2b-415c-b67c-8af4ff4e2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "# import seaborn as sns\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30750cda-cfa7-4fa6-b7c6-d0b01e4eb427",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1', natural=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe02d96-288e-435e-8791-b7d073fca71f",
   "metadata": {},
   "source": [
    "## Часть первая, с блекджеком и стратегиями"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5553a-5a4c-48f2-beec-24c64ed29262",
   "metadata": {},
   "source": [
    "**Задание 1**. Рассмотрим очень простую стратегию: говорить stand, если у нас на руках комбинация в 19, 20 или 21 очко, во всех остальных случаях говорить hit. \n",
    "Используйте методы Монте-Карло, чтобы оценить выигрыш от этой стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac5c2015-7bc4-454f-a84f-842e7782ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NUM = 500_000\n",
    "STAND = 0\n",
    "HIT = 1\n",
    "ACTIONS = {0: 'STAND', 1: 'HIT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d802ce0e-27d4-49b6-9364-c10e4b7314e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06701e227620455091023da7e1e75214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средний выигрыш от наивной стратегии: -0.19815\n"
     ]
    }
   ],
   "source": [
    "rewards = np.zeros(TASK_NUM)\n",
    "\n",
    "for task_id in trange(TASK_NUM):\n",
    "    state = env.reset()\n",
    "    is_done = False\n",
    "    card_sum = state[0]\n",
    "    while not is_done:\n",
    "        action = HIT if card_sum < 19 else STAND\n",
    "        state, reward, is_done, _ = env.step(action)\n",
    "        card_sum = state[0]\n",
    "    rewards[task_id] = reward\n",
    "\n",
    "print(f'Средний выигрыш от наивной стратегии: {rewards.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86e00f-fc96-4f24-9a2b-78831e2f844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(sorted(Counter(rewards).items()), columns=['reward', 'count'])\n",
    "sns.barplot(x='reward', y='count', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c9c81-4622-4d09-80ac-954a83a98eab",
   "metadata": {},
   "source": [
    "**Задание 2**. Реализуйте метод обучения с подкреплением без модели (можно Q-обучение, но рекомендую попробовать и другие, например Monte Carlo control) для обучения стратегии в блекджеке, используя окружение Blackjack-v0 из OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdec1981-e515-4ec9-b4d1-03aa96cb4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bdcd176-3ac5-41d2-b965-50e2739446fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = {0: 'STAND', 1: 'HIT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54315c81-4bfb-48e6-82d3-4d8a488e69c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_soft_policy(eps, Q, state):\n",
    "    coin = np.random.choice([0, 1], p=[eps, 1 - eps])\n",
    "    if coin == 0:\n",
    "        return np.random.randint(len(Q[state]['actions']))\n",
    "    else:\n",
    "        return np.argmax(Q[state]['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6160644-f1f2-4d31-9257-5ead9d1085ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(episode, gamma, Q, actions):\n",
    "    G = 0\n",
    "    for t in range(len(episode))[::-1]:\n",
    "        state, reward = episode[t]\n",
    "        G = gamma * G + reward\n",
    "        Q[state]['actions'][actions[t]] = (reward - Q[state]['actions'][actions[t]]) / (Q[state]['count'][actions[t]] + 1)\n",
    "        Q[state]['count'][actions[t]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1273250-9f50-43be-bca2-a30f95fa352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(eps, gamma, action_dim, environment, task_num):\n",
    "    Q = defaultdict(lambda: {'actions': np.zeros(action_dim), 'count': np.zeros(action_dim)})\n",
    "    rewards = np.zeros(task_num)\n",
    "    for task_id in trange(task_num):\n",
    "        state_prev = environment.reset()\n",
    "        episode = []\n",
    "        actions = []\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            action = epsilon_soft_policy(eps, Q, state_prev)\n",
    "            actions.append(action)\n",
    "            state, reward, is_done, _ = environment.step(action)\n",
    "            episode.append((state_prev, reward))\n",
    "            state_prev = state\n",
    "        rewards[task_id] = reward\n",
    "        update_q(episode, gamma, Q, actions)\n",
    "    return Q, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd610086-47a6-4ab0-a884-c61c5a18f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_params(action_dim, environment):\n",
    "    gammas = np.linspace(0.5, 1, 10)\n",
    "    epsilons = np.linspace(0.01, 0.5, 25)\n",
    "    best_reward = -np.inf\n",
    "\n",
    "    for gamma, eps in itertools.product(gammas, epsilons):\n",
    "        Q, rewards = run(eps, gamma, action_dim=action_dim, environment=environment, task_num=TASK_NUM)\n",
    "        if rewards.mean() > best_reward:\n",
    "            best_gamma = gamma\n",
    "            best_eps = eps\n",
    "            best_reward = rewards.mean()\n",
    "        print(f'Средний выигрыш при gamma={gamma}, eps={eps}: {rewards.mean()}')\n",
    "    return best_eps, best_gamma, best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f381b6a-cefb-4daf-ba70-3ba827f1b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_q(best_eps, best_gamma, action_dim, environment):\n",
    "    Q, rewards = run(eps=best_eps, gamma=best_gamma, action_dim=action_dim, environment=environment, task_num=500_000)\n",
    "    print(f'Средняя награда во время обучения: {rewards.mean()}')\n",
    "    rewards = np.zeros(TASK_NUM)\n",
    "    for task_id in trange(TASK_NUM):\n",
    "        state_prev = environment.reset()\n",
    "        episode = []\n",
    "        actions = []\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            action = epsilon_soft_policy(best_eps, Q, state_prev)\n",
    "            actions.append(action)\n",
    "            state, reward, is_done, _ = environment.step(action)\n",
    "            state_prev = state\n",
    "        rewards[task_id] = reward\n",
    "    print(f'Средняя награда во время инференса: {rewards.mean()}')\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4e66c-5a87-4b28-97d2-f00de97b168c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_eps, best_gamma, best_result = choose_best_params(action_dim=2, environment=env)\n",
    "print(f'Лучшие параметры: gamma={best_gamma}, eps={best_eps}, mean_reward={best_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ef6fbf-2d82-4a13-9bda-b10ff301f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gamma, best_eps = 1, 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "047b0147-e750-4c42-8d08-65f8a4a1b493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b807aac2dba245eaaa0dd8725408cefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда во время обучения: -0.18185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8075209f55642fb88f4695f52ef0b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда во время инференса: -0.16678\n"
     ]
    }
   ],
   "source": [
    "# eps = 0.05, gamma = 1\n",
    "Q = inference_q(best_eps, best_gamma, action_dim=2, environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a38713-f13c-4902-835f-734c5f1b2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_table(Q, usable_ace=True):\n",
    "    index = np.unique([current_sum for current_sum, dealer_first_card, ace in Q.keys() if ace == usable_ace])\n",
    "    index.sort()\n",
    "    columns = np.unique([dealer_first_card for current_sum, dealer_first_card, ace in Q.keys() if ace == usable_ace])\n",
    "    columns.sort()\n",
    "    describe_table = pd.DataFrame(columns=columns, index=index).fillna('nan')\n",
    "    count_table = pd.DataFrame(columns=columns, index=index).fillna(0)\n",
    "    for state, value in Q.items():\n",
    "        current_sum, dealer_first_card, ace = state\n",
    "        if not ace == usable_ace:\n",
    "            continue\n",
    "        action = np.argmax(value['actions'])\n",
    "        state_count = np.sum(value['count'])\n",
    "        describe_table.loc[current_sum, dealer_first_card] = ACTIONS[action]\n",
    "        count_table.loc[current_sum, dealer_first_card] = state_count\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(data=count_table.astype(int), annot=describe_table, fmt='s')\n",
    "    plt.title(f'Table for usable_ace={usable_ace}')\n",
    "    plt.xlabel('Dealers first card')\n",
    "    plt.ylabel('Current hand sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6d5b8-9739-4d3f-87b5-2965b38772a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_table(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c62516-eaad-4d13-8b9a-c163d9824eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_table(Q, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3253e6-5bf0-499e-a63b-610169e8f181",
   "metadata": {},
   "source": [
    "# TODO: add gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abd77257-5992-4f5d-97e7-a5d0dab14ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text.blackjack import BlackjackEnv, sum_hand, cmp, usable_ace, is_bust, is_natural, score\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33bcd113-4acd-43f1-a148-583c0a9bef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NUM = 500_000\n",
    "ACTIONS = {0: 'STAND', 1: 'HIT', 2: 'DOUBLE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86cccff4-16b1-4394-be0f-5486aa444884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleBlackjackEnv(BlackjackEnv):\n",
    "    def __init__(self, natural=False, sab=False):\n",
    "        super().__init__(natural=natural, sab=sab)\n",
    "        self.base_env = gym.make('Blackjack-v1', natural=True)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.base_env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 2:\n",
    "            # double — удвоить ставку\n",
    "            # при этом больше действий делать нельзя, игроку выдаётся ровно одна дополнительная карта, а выигрыш или проигрыш удваивается\n",
    "            state, reward, is_done, _ = self.base_env.step(1)\n",
    "            if is_done:\n",
    "                return state, reward * 2, is_done, _\n",
    "            else:\n",
    "                state, reward, is_done, _ = self.base_env.step(0)\n",
    "                return state, reward * 2, is_done, _\n",
    "        else:\n",
    "            return self.base_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00b36163-6daa-4d05-850a-1fcf4f80f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_double = DoubleBlackjackEnv(natural=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d100af-2749-475d-ae42-452a4060ac87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c608503136624cdabd68371370899b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда во время обучения: -0.197854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6c3ac090ec46a9b52b7e86008a3785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда во время инференса: -0.18814\n"
     ]
    }
   ],
   "source": [
    "# eps = 0.05, gamma = 1\n",
    "Q = inference_q(best_eps, best_gamma, action_dim=3, environment=env_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3ea5c-c535-47cf-8caa-b569347b29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_table(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7121128a-ac2c-486c-b12a-f2eb007a4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_table(Q, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa6219c2-61fe-44f0-9a2c-4fa06bf5f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoffmanBlackjackEnv(BlackjackEnv):\n",
    "    # 1 -> Ace, 10 -> Jack, Queen, King\n",
    "    DECK = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10] * 4\n",
    "    def __init__(self, natural=False, sab=False):\n",
    "        super().__init__(natural=natural, sab=sab)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.deck = self.DECK[:]\n",
    "        # Stanford Wong's counting scheme\n",
    "        self.counting_rule = {\n",
    "            1: -1, \n",
    "            2: 0.5, \n",
    "            3: 1, \n",
    "            4: 1, \n",
    "            5: 1.5, \n",
    "            6: 1, \n",
    "            7: 0.5,\n",
    "            8: 0,\n",
    "            9: -0.5,\n",
    "            10: -1,\n",
    "        }\n",
    "        self.deck_score = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        if len(self.deck) <= 15:\n",
    "            self.deck_score = 0\n",
    "            self.deck = self.DECK[:]\n",
    "        self.dealer = self.draw_hand()\n",
    "        self.player = self.draw_hand()\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def draw_hand(self):\n",
    "        return [self.draw_card(), self.draw_card()]\n",
    "    \n",
    "    def draw_card(self):\n",
    "        card = np.random.choice(self.deck)\n",
    "        self.deck.remove(card)\n",
    "        self.deck_score += self.counting_rule[card]\n",
    "        return card\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return (sum_hand(self.player), self.dealer[0], usable_ace(self.player), self.deck_score)\n",
    "        \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 2:\n",
    "            # double — удвоить ставку\n",
    "            # при этом больше действий делать нельзя, игроку выдаётся ровно одна дополнительная карта, а выигрыш или проигрыш удваивается\n",
    "            done = True\n",
    "            while sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(self.draw_card())\n",
    "            reward = cmp(score(self.player), score(self.dealer))\n",
    "            reward *= 2\n",
    "        elif action == 1:  # hit: add a card to players hand and return\n",
    "            self.player.append(self.draw_card())\n",
    "            if is_bust(self.player):\n",
    "                done = True\n",
    "                reward = -1.\n",
    "            else:\n",
    "                done = False\n",
    "                reward = 0.\n",
    "        else:  # stick: play out the dealers hand, and score\n",
    "            done = True\n",
    "            while sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(self.draw_card())\n",
    "            reward = cmp(score(self.player), score(self.dealer))\n",
    "            if self.sab and is_natural(self.player) and not is_natural(self.dealer):\n",
    "                # Player automatically wins. Rules consistent with S&B\n",
    "                reward = 1.0\n",
    "            elif (\n",
    "                not self.sab\n",
    "                and self.natural\n",
    "                and is_natural(self.player)\n",
    "                and reward == 1.0\n",
    "            ):\n",
    "                # Natural gives extra points, but doesn't autowin. Legacy implementation\n",
    "                reward = 1.5\n",
    "        return self._get_obs(), reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15b28627-ae2d-472b-aa01-8bfd96b298a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_hoffman = HoffmanBlackjackEnv(natural=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "513e2bc6-12a1-4bd5-a966-21e18e612dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d9b682cebe457cbc8bf583c4d1625a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда во время обучения: -0.110766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fe02cb78cc469d9d485d15c9fa3ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда во время инференса: -0.10225\n"
     ]
    }
   ],
   "source": [
    "# eps = 0.05, gamma = 1\n",
    "Q = inference_q(best_eps, best_gamma, action_dim=3, environment=env_hoffman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57df5ad-776a-44c4-b60e-19d7b7bc7ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps = 0.05, gamma = 1\n",
    "Q = run(EPSILON, GAMMA, action_dim=3, environment=env_hoffman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ddcc7-d9c4-4cd3-bec2-2bbd25fa6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_table(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e2c921-d8f6-48f4-bae2-8be5b1ba89d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
