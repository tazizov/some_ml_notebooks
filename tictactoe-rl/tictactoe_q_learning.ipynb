{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe_env import TicTacToe\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def strategy(self, state):\n",
    "        possible_steps = state[1]\n",
    "        return possible_steps[np.random.randint(possible_steps.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeGame:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def run_episode(self, player_x, player_o, return_history=False):\n",
    "        state, _, is_done, _ = self.env.reset()\n",
    "        states_x, states_o = [], []\n",
    "        rewards_x, rewards_o = [], []\n",
    "        actions_x, actions_o = [], []\n",
    "        players = [player_x, player_o]\n",
    "        cur_player = 0\n",
    "        while not is_done:\n",
    "            step = players[cur_player].strategy(state)\n",
    "            state, reward, is_done, _ = self.env.step(step)\n",
    "            if cur_player == 0:\n",
    "                rewards_x.append(reward)\n",
    "                states_x.append(state)\n",
    "                actions_x.append(step)\n",
    "            else:\n",
    "                if reward == -10:\n",
    "                    rewards_o.append(reward)\n",
    "                else:\n",
    "                    rewards_o.append(-reward)\n",
    "                states_o.append(state)\n",
    "                actions_o.append(step)\n",
    "            cur_player = (cur_player + 1) % 2\n",
    "        if (len(rewards_x) > len(rewards_o)) and (rewards_x[-1] == 1):\n",
    "            rewards_o[-1] = -1\n",
    "        if (rewards_o[-1] == 1):\n",
    "            rewards_x[-1] = -1\n",
    "        if return_history:\n",
    "            history_x = [(state, action, reward) for (state, action, reward) in zip(states_x, actions_x, rewards_x)]\n",
    "            history_o = [(state, action, reward) for (state, action, reward) in zip(states_o, actions_o, rewards_o)] \n",
    "            return history_x, history_o\n",
    "        return rewards_x[-1], rewards_o[-1]\n",
    "\n",
    "    def check_mean_reward(self, player_x, player_o, n_iter):\n",
    "        rewards_x = []\n",
    "        for _ in tqdm(range(n_iter)):\n",
    "            reward_x, _ = self.run_episode(player_x, player_o)\n",
    "            rewards_x.append(reward_x)\n",
    "        return np.mean(rewards_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec3ba59f56943dbaac1dde7f8a96c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.2748"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TicTacToe()\n",
    "game = TicTacToeGame(env)\n",
    "player_x = RandomPlayer()\n",
    "player_o = RandomPlayer()\n",
    "\n",
    "game.check_mean_reward(player_x, player_o, n_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningPlayer:\n",
    "    def __init__(self, eps, alpha, gamma, n_rows=3, n_cols=3):\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_rows * n_cols))\n",
    "        self.possible_actions = list(product(range(n_rows), range(n_cols)))\n",
    "        self.set_training()\n",
    "        self.total_episodes_training = 0\n",
    "    \n",
    "    def set_training(self):\n",
    "        self.training = True\n",
    "\n",
    "    def set_evaluating(self):\n",
    "        self.training = False\n",
    "\n",
    "    def update_q(self, episode_history):\n",
    "        # for t in range(len(episode_history)):\n",
    "        #     state, action, reward = episode_history[t] # here r[t + 1], s[t], a[t]\n",
    "        #     state_next, reward_next, _ = episode_history[t + 1] \n",
    "        #     # here we need reward[t + 1], s[t + 1]\n",
    "        #     self.q[state][action] += self.alpha * (reward + self.gamma * np.max(self.q[state_next][action])) - self.q[state][action]\n",
    "        self.total_episodes_training += 1\n",
    "        for episode_step in episode_history:\n",
    "            state, _, _ = episode_step\n",
    "            state_hash = state[0]\n",
    "            impossible_actions = np.where(np.array([*state_hash]) != '1')\n",
    "            self.q_table[state_hash][impossible_actions] = -np.inf\n",
    "        for t in range(1, len(episode_history)):\n",
    "            # to r[t - 1], s[t - 1]\n",
    "            state_cur, action_cur, _ = episode_history[t - 1]\n",
    "            state_next, _, reward_next = episode_history[t]\n",
    "            state_cur_hash, state_next_hash = state_cur[0], state_next[0]\n",
    "            \n",
    "            action_id = self.possible_actions.index(action_cur)\n",
    "            # here we need reward[t + 1], s[t + 1]\n",
    "            self.q_table[state_cur_hash][action_id] += self.alpha * (reward_next + self.gamma * np.max(self.q_table[state_next_hash][action_id]) - self.q_table[state_cur_hash][action_id])\n",
    "\n",
    "    def strategy(self, state):\n",
    "        state_hash = state[0]\n",
    "        coin = np.random.rand()\n",
    "        greedy_action = self.greedy_step(state_hash)\n",
    "        random_action = self.random_step()\n",
    "        if self.training:\n",
    "            coin = np.random.rand() < self.eps\n",
    "            if coin:\n",
    "                action = random_action\n",
    "            else:\n",
    "                action = greedy_action\n",
    "        else:\n",
    "            action = greedy_action\n",
    "        return action\n",
    "\n",
    "    def greedy_step(self, state_hash):\n",
    "        return self.possible_actions[self.q_table[state_hash].argmax()]\n",
    "    \n",
    "    def random_step(self):\n",
    "        return self.possible_actions[np.random.randint(0, len(self.possible_actions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplayer_params = {'eps': 0.1, 'alpha': 0.01, 'gamma': 1}\n",
    "qplayer_x , qplayer_o = [QLearningPlayer(**qplayer_params), QLearningPlayer(**qplayer_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_x = []\n",
    "# while not any(history_o):\n",
    "#     history_x, history_o = game.run_episode(qplayer_x, qplayer_o, True)\n",
    "# print(len(history_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bc3fbb941a417a89885330f619486d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tazizov\\AppData\\Local\\Temp/ipykernel_848/1255583138.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  self.q_table[state_cur_hash][action_id] += self.alpha * (reward_next + self.gamma * np.max(self.q_table[state_next_hash][action_id]) - self.q_table[state_cur_hash][action_id])\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    history_x, history_o = game.run_episode(qplayer_x, qplayer_o, True)\n",
    "    \n",
    "    qplayer_x.update_q(history_x)\n",
    "    qplayer_o.update_q(history_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8721ea6a9da04d6994501678d48621b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.67696"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.check_mean_reward(qplayer_x, qplayer_o, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10, 0)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.run_episode(qplayer, qplayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af256a2c14764f8d9fa3fd1a707730eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.98386"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.check_mean_reward(qplayer_x, player_o, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_x = qplayer_x.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([item[1] for item in list(q_table_x.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a96679fb6303d6bfe882256a806ccdfeeac76400b462e2d8dedd73c503063ab7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('base_ml_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
