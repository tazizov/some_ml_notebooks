{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe_env import TicTacToe\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def strategy(self, state):\n",
    "        possible_steps = state[1]\n",
    "        return possible_steps[np.random.randint(possible_steps.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeGame:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def run_episode(self, player_x, player_o, return_history=False):\n",
    "        state, _, is_done, _ = self.env.reset()\n",
    "        states_x, states_o = [], []\n",
    "        rewards_x, rewards_o = [], []\n",
    "        actions_x, actions_o = [], []\n",
    "        players = [player_x, player_o]\n",
    "        cur_player = 0\n",
    "        while not is_done:\n",
    "            step = players[cur_player].strategy(state)\n",
    "            state, reward, is_done, _ = self.env.step(step)\n",
    "            if cur_player == 0:\n",
    "                rewards_x.append(reward)\n",
    "                states_x.append(state)\n",
    "                actions_x.append(step)\n",
    "            else:\n",
    "                rewards_o.append(-reward)\n",
    "                states_o.append(state)\n",
    "                actions_o.append(step)\n",
    "            cur_player = (cur_player + 1) % 2\n",
    "        if (len(rewards_x) > len(rewards_o)) and (rewards_x[-1] == 1):\n",
    "            rewards_o[-1] = -1\n",
    "        if (rewards_o[-1] == 1):\n",
    "            rewards_x[-1] = -1\n",
    "        if return_history:\n",
    "            history_x = [(state, action, reward) for (state, action, reward) in zip(states_x, actions_x, rewards_x)]\n",
    "            history_o = [(state, action, reward) for (state, action, reward) in zip(states_o, actions_o, rewards_o)] \n",
    "            return history_x, history_o\n",
    "        return rewards_x[-1], rewards_o[-1]\n",
    "\n",
    "    def check_mean_reward(self, player_x, player_o, n_iter):\n",
    "        rewards_x = []\n",
    "        for _ in tqdm(range(n_iter)):\n",
    "            reward_x, _ = self.run_episode(player_x, player_o)\n",
    "            rewards_x.append(reward_x)\n",
    "        return np.mean(rewards_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fe44bd03804a489f44f0779dc308a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3052"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TicTacToe()\n",
    "game = TicTacToeGame(env)\n",
    "player_x = RandomPlayer()\n",
    "player_o = RandomPlayer()\n",
    "\n",
    "game.check_mean_reward(player_x, player_o, n_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningPlayer:\n",
    "    def __init__(self, eps, alpha, gamma, n_rows=3, n_cols=3):\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_rows * n_cols))\n",
    "        self.possible_actions = list(product(range(n_rows), range(n_cols)))\n",
    "        self.set_training()\n",
    "        self.total_episodes_training = 0\n",
    "    \n",
    "    def set_training(self):\n",
    "        self.training = True\n",
    "\n",
    "    def set_evaluating(self):\n",
    "        self.training = False\n",
    "\n",
    "    def update_q(self, episode_history):\n",
    "        # for t in range(len(episode_history)):\n",
    "        #     state, action, reward = episode_history[t] # here r[t + 1], s[t], a[t]\n",
    "        #     state_next, reward_next, _ = episode_history[t + 1] \n",
    "        #     # here we need reward[t + 1], s[t + 1]\n",
    "        #     self.q[state][action] += self.alpha * (reward + self.gamma * np.max(self.q[state_next][action])) - self.q[state][action]\n",
    "        self.total_episodes_training += 1\n",
    "        for t in range(1, len(episode_history)):\n",
    "            # to r[t - 1], s[t - 1]\n",
    "            state_cur, action_cur, _ = episode_history[t - 1]\n",
    "            state_next, _, reward_next = episode_history[t]\n",
    "            state_cur_hash, state_next_hash = state_cur[0], state_next[0]\n",
    "            action_id = self.possible_actions.index(action_cur)\n",
    "            # here we need reward[t + 1], s[t + 1]\n",
    "            self.q_table[state_cur_hash][action_id] += self.alpha * (reward_next +\\\n",
    "                                 self.gamma * np.max(self.q_table[state_next_hash][action_id]) - self.q_table[state_cur_hash][action_id])\n",
    "\n",
    "    def strategy(self, state):\n",
    "        state_hash = state[0]\n",
    "        coin = np.random.rand()\n",
    "        greedy_action = self.greedy_step(state_hash)\n",
    "        random_action = self.random_step()\n",
    "        if self.training:\n",
    "            coin = np.random.rand() < self.eps\n",
    "            if coin:\n",
    "                action = random_action\n",
    "            else:\n",
    "                action = greedy_action\n",
    "        else:\n",
    "            action = greedy_action\n",
    "        return action\n",
    "\n",
    "    def greedy_step(self, state_hash):\n",
    "        return self.possible_actions[self.q_table[state_hash].argmax()]\n",
    "    \n",
    "    def random_step(self):\n",
    "        return self.possible_actions[np.random.randint(0, len(self.possible_actions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplayer_params = {'eps': 0.1, 'alpha': 0.01, 'gamma': 1}\n",
    "qplayer_x , qplayer_o = [QLearningPlayer(**qplayer_params), QLearningPlayer(**qplayer_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_x = []\n",
    "# while not any(history_o):\n",
    "#     history_x, history_o = game.run_episode(qplayer_x, qplayer_o, True)\n",
    "# print(len(history_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_x, history_o = game.run_episode(qplayer_x, qplayer_o, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[el[2] for el in history_o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplayer_x.update_q(history_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.QLearningPlayer.__init__.<locals>.<lambda>()>,\n",
       "            {'111111111': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qplayer_x.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[el[2] for el in history_o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplayer_o.update_q(history_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('211111111',\n",
       "   array([[0, 1],\n",
       "          [0, 2],\n",
       "          [1, 0],\n",
       "          [1, 1],\n",
       "          [1, 2],\n",
       "          [2, 0],\n",
       "          [2, 1],\n",
       "          [2, 2]], dtype=int64),\n",
       "   -1),\n",
       "  (0, 0),\n",
       "  0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_history = history_o\n",
    "for t in range(1, len(episode_history)):\n",
    "    state_cur, action_cur, _ = episode_history[t - 1]\n",
    "    state_next, _, reward_next = episode_history[t]\n",
    "    state_cur_hash, state_next_hash = state_cur[0], state_next[0] \n",
    "    action_id = 0\n",
    "    print(state_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('211111111',\n",
       "   array([[0, 1],\n",
       "          [0, 2],\n",
       "          [1, 0],\n",
       "          [1, 1],\n",
       "          [1, 2],\n",
       "          [2, 0],\n",
       "          [2, 1],\n",
       "          [2, 2]], dtype=int64),\n",
       "   -1),\n",
       "  (0, 0),\n",
       "  10)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.QLearningPlayer.__init__.<locals>.<lambda>()>,\n",
       "            {'211111111': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qplayer_o.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440788710ba04428814c20daa03732b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3260/1047424783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_mean_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqplayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqplayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3260/2508879411.py\u001b[0m in \u001b[0;36mcheck_mean_reward\u001b[1;34m(self, player_x, player_o, n_iter)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mrewards_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mreward_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer_o\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mrewards_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3260/2508879411.py\u001b[0m in \u001b[0;36mrun_episode\u001b[1;34m(self, player_x, player_o, return_history)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mcur_player\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur_player\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcur_player\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3260/2769283694.py\u001b[0m in \u001b[0;36mstrategy\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mstate_hash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mcoin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mgreedy_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgreedy_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_hash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mrandom_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3260/2769283694.py\u001b[0m in \u001b[0;36mgreedy_step\u001b[1;34m(self, state_hash)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgreedy_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_hash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_hash\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrandom_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game.check_mean_reward(qplayer, qplayer, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10, 0)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.run_episode(qplayer, qplayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a96679fb6303d6bfe882256a806ccdfeeac76400b462e2d8dedd73c503063ab7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('base_ml_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
