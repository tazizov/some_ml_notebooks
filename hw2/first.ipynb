{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "import itertools\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Филигранно распикливаем ([с возможной утечкой памяти:)](https://stackoverflow.com/questions/7395542/is-explicitly-closing-files-important))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_dict = pickle.load(open('data/chgk/players.pkl', 'rb'))\n",
    "results_dict = pickle.load(open('data/chgk/results.pkl', 'rb'))\n",
    "tournaments_dict = pickle.load(open('data/chgk/tournaments.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тренировочном наборе оставляем только турниры, у которых `dateStart` 2019, в тестовый набор - турниры, у которых `dateStart` 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фильтруем `results`, оставляем только те, где есть `mask` в ключах (без повопросных результатов обучать силу проблематично)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filtered_train = {}\n",
    "results_filtered_test = {}\n",
    "for k, res in results_dict.items():\n",
    "    t_res_new = []\n",
    "    for t_res in results_dict[k]:\n",
    "        if 'mask' in t_res.keys():\n",
    "            if t_res['mask'] is not None:\n",
    "                if 'X' not in t_res['mask'] and '?' not in t_res['mask']:\n",
    "                    t_res_new.append(t_res)\n",
    "    if t_res_new:\n",
    "        if tournaments_dict[k]['dateStart'].startswith('2019'):\n",
    "            results_filtered_train[k] = t_res_new\n",
    "        if tournaments_dict[k]['dateStart'].startswith('2020'):\n",
    "            results_filtered_test[k] = t_res_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также понадобится правильное число вопросов в каждом турнире, чтобы фильтровать результаты по длине маски\n",
    "(иначе получится так, что разные команды в рамках одного турнира ответили на разное количество вопросов, а значит не получится восстановить однозначное соответствие `команда`-`вопрос`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_question_count = {}\n",
    "\n",
    "for k, v in results_filtered_train.items():\n",
    "    tournament_question_count[k]= max([len(t_res['mask']) for t_res in v])\n",
    "    \n",
    "for k, v in results_filtered_test.items():\n",
    "    tournament_question_count[k]= max([len(t_res['mask']) for t_res in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filtered_train_len = {}\n",
    "results_filtered_test_len = {}\n",
    "train_players = set()\n",
    "for k, v in results_filtered_train.items():\n",
    "    t_res_ = []\n",
    "    for t_res in v:\n",
    "        if len(t_res['mask']) == tournament_question_count[k]:\n",
    "            train_players.update([m['player']['id'] for m in t_res['teamMembers']])\n",
    "            t_res_.append(t_res)\n",
    "    results_filtered_train_len[k] = t_res_\n",
    "\n",
    "for k, v in results_filtered_test.items():\n",
    "    t_res_ = []\n",
    "    for t_res in v:\n",
    "        if len(t_res['mask']) == tournament_question_count[k]:\n",
    "            t_res_.append(t_res)\n",
    "    results_filtered_test_len[k] = t_res_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делая наивное предположение что если команда ответила на вопрос, то каждый игрок в этой команде ответил на вопрос, собираем большую таблицу взаимодействий `игрок-вопрос-[ответил/не ответил]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_df = pd.DataFrame(columns=['pid', 'qid', 'tournament', 'tid', 'res'])\n",
    "\n",
    "pid = []\n",
    "qid = []\n",
    "res = []\n",
    "tournament = []\n",
    "tid = []\n",
    "for k, v in results_filtered_train_len.items():\n",
    "    for t_res in v:\n",
    "        members = [m['player']['id'] for m in t_res['teamMembers']]\n",
    "        t_pid = list(itertools.chain.from_iterable(itertools.repeat(m, tournament_question_count[k]) for m in members))\n",
    "        pid.extend(t_pid)\n",
    "        t_qid = [f'{k}_{i}' for i in range(tournament_question_count[k])] * len(members)\n",
    "        qid.extend(t_qid)\n",
    "        tid.extend([t_res['team']['id']] * len(t_qid))\n",
    "        tournament.extend([k] * len(t_qid))\n",
    "        res.extend(list(map(int, t_res['mask'])) * len(members))\n",
    "\n",
    "pq_df['pid'] = np.int32(pid)\n",
    "pq_df['qid'] = qid\n",
    "pq_df['tournament'] = tournament\n",
    "pq_df['tid'] = tid\n",
    "pq_df['res'] = np.int8(res)\n",
    "pq_df['qid'] = pq_df['qid'].astype(str)\n",
    "pq_df['pid'] = pq_df['pid'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь `tournament_results_train` и `tournament_results_t_test` нужен для тестирования полученных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_results_train = {}\n",
    "for tourn_id, t_res in results_filtered_train_len.items():\n",
    "    tids, names, positions, qtotals, members = [], [], [], [], []\n",
    "    for team in t_res:\n",
    "        members.append([m['player']['id'] for m in team['teamMembers']])\n",
    "        tids.append(team['team']['id'])\n",
    "        names.append(team['team']['name'])\n",
    "        positions.append(team['position'])\n",
    "        qtotals.append(team['questionsTotal'])\n",
    "        \n",
    "    tournament_results_train[tourn_id] = pd.DataFrame(\n",
    "        {\n",
    "        'tid': tids,\n",
    "        'name': names,\n",
    "        'position': positions,\n",
    "        'qtotal': qtotals,\n",
    "        'members': members\n",
    "        }\n",
    "    ).sort_values(by='position')\n",
    "    \n",
    "\n",
    "tournament_results_test = {}\n",
    "for tourn_id, t_res in results_filtered_test_len.items():\n",
    "    tids, names, positions, qtotals, members = [], [], [], [], []\n",
    "    for team in t_res:\n",
    "        t_members = []\n",
    "        for m in team['teamMembers']:\n",
    "            if m['player']['id'] in train_players:\n",
    "                t_members.append(m['player']['id'])\n",
    "        if t_members:\n",
    "            members.append(t_members)\n",
    "            tids.append(team['team']['id'])\n",
    "            names.append(team['team']['name'])\n",
    "            positions.append(team['position'])\n",
    "            qtotals.append(team['questionsTotal'])\n",
    "    tournament_results_test[tourn_id] = pd.DataFrame(\n",
    "        {\n",
    "        'tid': tids,\n",
    "        'name': names,\n",
    "        'position': positions,\n",
    "        'qtotal': qtotals,\n",
    "        'members': members\n",
    "        }\n",
    "    ).sort_values(by='position')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предположения:\n",
    "\n",
    "- игрок ответил на вопрос == команда в которой был игрок ответила на вопрос\n",
    "\n",
    "- вероятность того, что игрок ответит на вопрос зависит от сложности вопроса и умений игрока, и никак не зависит от того в какой он команде\n",
    "\n",
    "__Тогда__:\n",
    "\n",
    "Событие $A_{pq}$ == p-игрок ответил на q-ый вопрос\n",
    "\n",
    "В таком случае\n",
    "\n",
    "$P(A_{pq}) = \\sigma(skill_p + difficult_q)$\n",
    "\n",
    "Таким образом, из повопросного датасета можно выучить `skill_p` и `difficult_q` (просто записав лолгосс и минимизировать его градиентным спуском)\n",
    "\n",
    "Однако, очень удобно (для того чтобы использовать библиотечные солверы и прочие радости) кодировать $A_{pq}$ с помощью `OneHotEncoding`. В таком случае, \n",
    "если $x_i$ = `[0, 0, ..., [1 на месте p], 0, 0, ... [1 на месте q]]`, умножить его скалярно на вектор `np.concat([skills, difficults])` это то же самое, что сложить `skill_p + difficult_q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=10, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=True, categories=[pq_df['pid'].unique().tolist(), pq_df['qid'].unique().tolist()])\n",
    "encoder.fit(pq_df[['pid', 'qid']])\n",
    "pq_df_oh = encoder.transform(pq_df[['pid', 'qid']])\n",
    "clf = LogisticRegression(solver='lbfgs', n_jobs=10)\n",
    "clf.fit(pq_df_oh, pq_df['res'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим рейтинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tournaments_rating_n_corrs(encoder, player_q_df, weights, t_results_test):\n",
    "    t_results_test_ = t_results_test.copy()\n",
    "    feature_names = encoder.get_feature_names(pq_df.columns[:2])\n",
    "    players = feature_names[[fname.startswith('pid') for fname in feature_names]]\n",
    "    players = np.array(list(map(lambda x: np.int32(x.replace('pid_','')), players)))\n",
    "    players_scores = pd.DataFrame({'player_id': players, 'score': weights[:len(players)]}).set_index('player_id')\n",
    "    corrs_spearman = []\n",
    "    corrs_kendall = []\n",
    "    for t_id, t_result in t_results_test_.items():\n",
    "        t_result['predicted_score'] = t_result['members'].apply(lambda x: players_scores.loc[x]['score'].mean())\n",
    "        corrs_spearman.append(np.abs(t_result[['position', 'predicted_score']].corr(method='spearman')['position']['predicted_score']))\n",
    "        corrs_kendall.append(np.abs(t_result[['position', 'predicted_score']].corr(method='kendall')['position']['predicted_score']))\n",
    "    return t_results_test_, np.mean(corrs_spearman), np.mean(corrs_kendall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_results_pred, spearman, kendall = get_tournaments_rating_n_corrs(encoder, pq_df, clf.coef_[0], tournament_results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7444559048119628\n",
      "0.5867546216490609\n"
     ]
    }
   ],
   "source": [
    "print(spearman)\n",
    "print(kendall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM - схема"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку ЧГК - командная игра, кажется не слишком разумным что если игрок взял вопрос вместе с какой-то командой, то он сам ответил на этот вопрос. Нужно как-то учитывать выступление одного и того же игрока в разных командах, другими словами, как и сказано в задании, переменные “игрок X ответил на вопрос Y” при условии данных должны стать зависимыми для игроков одной и той же команды\n",
    "\n",
    "__Предлагаемая EM-схема__:\n",
    "\n",
    "Будем использовать результат игрока $p$ на вопросе $q$ в качестве скрытой переменной (`res_pq`). Тогда\n",
    "\n",
    "* __E-шаг__:\n",
    "\n",
    "На $E$-шаге оцениваем мат-ожидание выбранной скрытой переменной. Предположим, что если хотя бы один игрок ответил на вопрос верно, то команда ответит на вопрос верно.\n",
    "\n",
    "Тогда\n",
    "\n",
    "$$\\mathop{\\mathbb{E}}(res_{pq}) = {p(A_{pq} | A_{tq})} = \\frac{\\sigma(skill_p + difficult_q)}{1 - \\prod_{p \\in t}(1 - \\sigma(skill_p + difficult_q))}$$, \n",
    "если команда ответила на вопрос, и $0$, если не ответила\n",
    "\n",
    "\n",
    "* __M-шаг__:\n",
    "\n",
    "На $M$-шаге просто учим логистическую регрессию на оцененные матожидания (правда я так и не понял как модель из `sklearn` заставить минимизировать лосс с мягкими метками, поэтому в качестве \"обучения логистической регрессии\" просто будем делать несколько шагов в сторону градиента правдоподобия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, weights):\n",
    "    return 1 / (1 + np.exp(-X.dot(weights)))\n",
    "\n",
    "def log_likelihood(expectations, pq_df_oh, weights):\n",
    "    sigma = sigmoid(pq_df_oh, weights)\n",
    "    return np.sum(expectations * np.log(sigma) + (1 - expectations) * np.log(1 - sigma))\n",
    "    \n",
    "def weights_gradient(weights, expectations, pq_df_oh):\n",
    "    return (csr_matrix(expectations - sigmoid(pq_df_oh, weights)).dot(pq_df_oh)).toarray()[0] / np.array((pq_df_oh != 0).sum(axis=0)).ravel()\n",
    "\n",
    "def Expectation(weights, pq_df, pq_df_oh):\n",
    "    pq_df_ = pq_df.copy()\n",
    "    sigma = sigmoid(pq_df_oh, weights)\n",
    "    pq_df_['sigma'] = sigma\n",
    "    pq_df_['one_sigma'] = 1 - sigma\n",
    "    prods_series = 1 - pq_df_.groupby(['tournament', 'tid', 'qid'])['one_sigma'].prod().rename('prod')\n",
    "    pq_df_ = pq_df_.merge(prods_series, on=['tournament', 'tid', 'qid'])\n",
    "    expectations = (pq_df_['sigma'] / (pq_df_['prod'])).values\n",
    "    expectations[pq_df_['res'] == 0] = 0    \n",
    "    return expectations\n",
    "\n",
    "def Maximization(expectations, pq_df_oh, weights):\n",
    "    weights_ = weights\n",
    "    for i in range(30):\n",
    "        weights_ += 1.5 * weights_gradient(weights_, expectations, pq_df_oh)\n",
    "    return weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.rand(pq_df_oh.shape[1])\n",
    "em_df = pd.DataFrame(columns=['iter', 'log_ll', 'Kendall_corr', 'Spearman_corr'])\n",
    "for i in range(10):\n",
    "    expectations = Expectation(weights, pq_df, pq_df_oh)\n",
    "    weights = Maximization(expectations, pq_df_oh, weights)\n",
    "    rating, corr_spearman, corr_kendall = get_tournaments_rating_n_corrs(encoder, pq_df, weights, tournament_results_test)\n",
    "    em_df = em_df.append(\n",
    "       pd.DataFrame(\n",
    "        {\n",
    "        'iter': [i + 1],\n",
    "        'log_ll': [log_likelihood(expectations, pq_df_oh, weights) / pq_df_oh.shape[0]],\n",
    "        'Kendall_corr': [corr_kendall],\n",
    "        'Spearman_corr' : [corr_spearman]\n",
    "    }), ignore_index=True)\n",
    "    clear_output()\n",
    "    display(em_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рейтинг лист турниров по сложности вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tournaments_difficult_rating(weights, pq_df, encoder):\n",
    "    feature_names = encoder.get_feature_names(pq_df.columns[:2])\n",
    "    questions = feature_names[[fname.startswith('qid') for fname in feature_names]]\n",
    "    questions = np.array(list(map(lambda x: x.replace('qid_',''), questions)))\n",
    "    questions_scores = pd.DataFrame({'qid': questions , 'score': weights[-len(questions):]})\n",
    "    train_tournaments_diffs = pq_df.merge(questions_scores, on='qid').groupby('tournament')['score'].mean()\n",
    "    diffs = []\n",
    "    names = []\n",
    "    tids = []\n",
    "    for tournament_id in train_tournaments_diffs.keys():\n",
    "        diffs.append(train_tournaments_diffs[tournament_id])\n",
    "        names.append(tournaments_dict[tournament_id]['name'])\n",
    "        tids.append(tournament_id)\n",
    "    tournaments_diff_rating = pd.DataFrame({\n",
    "        'tid' : tids,\n",
    "        'name': names,\n",
    "        'diff': diffs\n",
    "    }).sort_values(by='diff')\n",
    "    tournaments_diff_rating['place'] = tournamets_diff_rating.reset_index().index + 1\n",
    "    tournaments_diff_rating = tournaments_diff_rating[['name', 'place']].set_index('place')\n",
    "    return tournaments_diff_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tournaments_difficult_rating(clf.coef_[0], pq_df, encoder).head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
